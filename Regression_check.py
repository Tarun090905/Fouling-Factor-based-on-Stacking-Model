# -*- coding: utf-8 -*-
"""Fouling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cN3ElEk91S1QtnMUuAoGCogw3vQZcrXo
"""

import pandas as pd

# Pandas is a powerful library for data manipulation and analysis

from google.colab import drive
drive.mount('/content/drive')

# Replace with your actual file path
file_path = '/content/drive/My Drive/Python/data.xlsx'

# Read the Excel file
data = pd.read_excel(file_path)

# Now we drop Referance column
data=data.drop(columns=['Reference'])

# Show the content
print(data.head())

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
# which is built on top of Matplotlib and provides a high-level interface for drawing attractive statistical graphics.

import matplotlib.pyplot as plt
# fundamental plotting library, and pyplot provides a convenient way to create a wide variety of plots and visualizations.

# %matplotlib inline

# 热力图
plt.figure(figsize=(20,20))
plt.rcParams.update({"font.size":8})
sns.heatmap(data.corr(),annot=True,fmt='.1f',square=True)
plt.savefig('jjj')
plt.show()

# First We remove our Result Data
X=data.drop(columns=['Fouling factor (m2 K/kW)'])

# and put into another array
y=data['Fouling factor (m2 K/kW)']

from sklearn.preprocessing import StandardScaler
# StandardScaler  is a preprocessing technique used to standardize features by removing the mean and scaling to unit variance.
# it can help improve the performance of certain algorithms that are sensitive to the scale of the input features.

stand=StandardScaler()

X=stand.fit_transform(X)

# X is now standardized, meaning each feature will have a mean of 0 and a standard deviation of 1.

from sklearn.model_selection import train_test_split

#  split your dataset into random train and test subsets

tx,vx,ty,vy=train_test_split(X,y,test_size=0.2,random_state=666,shuffle=True)

# traning_X , Test_X , Traning_y , Test_y
# This means that the sequence of random numbers generated will be the same every time you run the code with that same value.





# First Logistic Regression

from sklearn.linear_model import LogisticRegression

log=LogisticRegression(C=0.1,n_jobs=-1,multi_class='ovr',random_state=101)

model=log.fit(tx,ty.astype('int'))

# this line is telling the log object to learn from the training data (tx and ty) and produce a trained model object (model) that can then be used for making predictions.

yp=model.predict(vx)

# Predict Fouling Factor w.r.t. Test Data of X.
# the trained model to make predictions on the test set of X.



from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score

MAE1=mean_absolute_error(vy,yp)

# The absolute differences between the predicted values and the actual values.

MAE1

MSE1=mean_squared_error(vy,yp)

# the average of the squared differences between the predicted values and the actual values.

MSE1

R21=r2_score(vy,yp)

# the proportion of the variance in the dependent variable that is predictable from the independent variables.
# An R2 of 1 means the model perfectly predicts the target variable.
# An R2 of 0 means the model performs no better than simply predicting the mean of the target variable.
# A negative R2 score indicates that the model is performing worse than predicting the mean.

R21



import numpy as np

# to calculate RAE and MAPE, which involve array operations.  # Multi-Dimensional Array

errors = np.abs(vy - yp)
mean_true = np.mean(vy)
RAE1 = np.mean(errors) / mean_true

RAE1

# Relative Absolute Error

ape = np.abs((vy - yp) / vy)
MAPE1 = np.mean(ape) * 100

MAPE1

# Mean Absolute Percentage Error





# Second Random Forest Regression

from sklearn.ensemble import RandomForestRegressor

rf=RandomForestRegressor(n_jobs=-1,random_state=111)

model=rf.fit(tx,ty)

yp=model.predict(vx)

MAE2=mean_absolute_error(vy,yp)

MAE2

MSE2=mean_squared_error(vy,yp)

MSE2

R22=r2_score(vy,yp)

R22

ape = np.abs((vy - yp) / vy)
MAPE2 = np.mean(ape) * 100

MAPE2

errors = np.abs(vy - yp)
mean_true = np.mean(vy)
RAE2 = np.mean(errors) / mean_true

RAE2





# Third XG Boost

import xgboost as xgb
# read data into Xgboost DMatrix format
#整体
train = xgb.DMatrix(tx, ty)
test = xgb.DMatrix(vx, vy)

# specify parameters via map
params_xgb={
    'booster':'gbtree',     #  tree-based models
    'objective': 'reg:squarederror',
    'eval_metric':'rmse',
    'eta': 0.1,             # Same to learning rate
    'gamma':0,              # Similar to min_impurity_decrease in GBDT
    'alpha': 0,            # L1 regularization term on weight (analogous to Lasso regression)
    'lambda': 2,          # L2 regularization term on weights (analogous to Ridge regression)
    'max_depth': 6,         # Same as the max_depth of GBDT
    'subsample': 1,         # Same as the subsample of GBDT
    'colsample_bytree': 1,  # Similar to max_features in GBM
    'min_child_weight': 1,  # minimum sum of instance weight (Hessian) needed in a child
    'nthread':1,            # default to maximum number of threads available if not set
}

watchlist=[(test,'eval')]
num_round = 10000
bst = xgb.train(params_xgb, train, num_round,verbose_eval=50,evals=watchlist,early_stopping_rounds=50)

yp=bst.predict(test)

MAE3=mean_absolute_error(vy,yp)

MAE3

MSE3=mean_squared_error(vy,yp)

MSE3

R23=r2_score(vy,yp)

R23

ape = np.abs((vy - yp) / vy)
MAPE3 = np.mean(ape) * 100

MAPE3

errors = np.abs(vy - yp)
mean_true = np.mean(vy)
RAE3 = np.mean(errors) / mean_true

RAE3





# Fourth Bagging Regression

from sklearn.ensemble import BaggingRegressor

bag=BaggingRegressor(n_jobs=-1,random_state=666)

model=bag.fit(tx,ty)

yp=model.predict(vx)

MSE4=mean_squared_error(vy,yp)

MSE4

MAE4=mean_absolute_error(vy,yp)

MAE4

R24=r2_score(vy,yp)

R24

errors = np.abs(vy - yp)
mean_true = np.mean(vy)
RAE4 = np.mean(errors) / mean_true

RAE4

ape = np.abs((vy - yp) / vy)
MAPE4 = np.mean(ape) * 100

MAPE4





# Fifth KNN Regression

from sklearn.neighbors import KNeighborsRegressor

KNN=KNeighborsRegressor(n_neighbors=5,n_jobs=-1)

KNN.fit(tx,ty)

yp=KNN.predict(vx)

MAE5=mean_absolute_error(vy,yp)

MAE5

MSE5=mean_squared_error(vy,yp)

MSE5

R25=r2_score(vy,yp)

R25

errors = np.abs(vy - yp)
mean_true = np.mean(vy)
RAE5 = np.mean(errors) / mean_true

RAE5

ape = np.abs((vy - yp) / vy)
MAPE5 = np.mean(ape) * 100

MAPE5





# Sixth Bayesian Ridge

from sklearn.linear_model import BayesianRidge

regressor = BayesianRidge()
# 拟合模型
BY=regressor.fit(tx, ty)
# 进行预测
yp = BY.predict(vx)

MAE6=mean_absolute_error(vy,yp)

MAE6

MSE6=mean_squared_error(vy,yp)

MSE6

R26=r2_score(vy,yp)

R26

errors = np.abs(vy - yp)
mean_true = np.mean(vy)
RAE6 = np.mean(errors) / mean_true

RAE6

ape = np.abs((vy - yp) / vy)
MAPE6 = np.mean(ape) * 100

MAPE6





# Seventh Gaussian Process Regression

import numpy as np
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF

# 定义高斯过程回归模型
kernel = RBF(length_scale=1.0)  # 核函数
regressor = GaussianProcessRegressor(kernel=kernel)

# 拟合模型
regressor.fit(tx,ty)

yp, sigma = regressor.predict(vx, return_std=True)

print("预测结果:", yp)
print("标准差:", sigma)

MAE7=mean_absolute_error(vy,yp)

MAE7

MSE7=mean_squared_error(vy,yp)

MSE7

R27=r2_score(vy,yp)

R27

errors = np.abs(vy - yp)
mean_true = np.mean(vy)
RAE7 = np.mean(errors) / mean_true

RAE7

ape = np.abs((vy - yp) / vy)
MAPE7 = np.mean(ape) * 100

MAPE7









# Eighth SVR

from sklearn.svm import SVR

svr=SVR().fit(tx,ty)
yp=svr.predict(vx)

MAE8=mean_absolute_error(vy,yp)

MAE8

MSE8=mean_squared_error(vy,yp)

MSE8

R28=r2_score(vy,yp)

R28

errors = np.abs(vy - yp.ravel())
mean_true = np.mean(vy)
RAE8 = np.mean(errors) / mean_true

RAE8

ape = np.abs((vy - yp.ravel()) / vy)
MAPE8 = np.mean(ape) * 100

MAPE8





# Ninth LGB

#整体
import lightgbm as lgb
train_data = lgb.Dataset(tx, ty)
test_data = lgb.Dataset(vx, vy)

# specify parameters via map
params_lgb = {
    'boost':'gbdt',
    'max_depth': -1,                # Same to max_depth of xgboost
    'application':'regression',     # Same to objective of xgboost
    'learning_rate': 0.1,           # Same to eta of xgboost
    'lambda_l2': 2,                 # Same to lambda of xgboost
    'min_data_in_leaf': 20,         # Same to min_samples_leaf of GBDT
    'bagging_fraction': 1.0,        # Same to subsample of xgboost
    'feature_fraction': 1.0,         # Same to colsample_bytree of xgboost
    'min_sum_hessian_in_leaf': 1e-3, # Same to min_child_weight of xgboost
    'num_threads': 1,
    'metric':'mae'
}

num_round = 1000
best = lgb.train(params_lgb, train_data, num_round, callbacks=[lgb.log_evaluation(period=50), lgb.early_stopping(stopping_rounds=50)], valid_sets=test_data)

yp=best.predict(vx)

MAE9=mean_absolute_error(vy,yp)

MAE9

MSE9=mean_squared_error(vy,yp)

MSE9

R29=r2_score(vy,yp)

R29

errors = np.abs(vy - yp.ravel())
mean_true = np.mean(vy)
RAE9 = np.mean(errors) / mean_true

RAE9

ape = np.abs((vy - yp.ravel()) / vy)
MAPE9 = np.mean(ape) * 100

MAPE9





# Tenth MLP Regression

from sklearn.neural_network import MLPRegressor

MLP=MLPRegressor(hidden_layer_sizes=(100,50),activation='relu',solver='adam')

MLP.fit(tx,ty)

yp=MLP.predict(vx)

MAE10=mean_absolute_error(vy,yp)

MAE10

MSE10=mean_squared_error(vy,yp)

MSE10

R210=r2_score(vy,yp)

R210

errors = np.abs(vy - yp)
mean_true = np.mean(vy)
RAE10 = np.mean(errors) / mean_true

RAE10

ape = np.abs((vy - yp) / vy)
MAPE10 = np.mean(ape) * 100

MAPE10





# Eleventh Sequential

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense,SimpleRNN,GRU
from tensorflow.keras.optimizers import Adam

model = Sequential()
model.add(LSTM(units=64, activation='relu', input_shape=(7,1)))
model.add(Dense(units=1))

# 编译模型
model.compile(optimizer=Adam(), loss='mean_squared_error')

model.fit(tx, ty, epochs=150, batch_size=32)

yp=model.predict(vx)

MAE11=mean_absolute_error(vy,yp)

MAE11

MSE11=mean_squared_error(vy,yp)

MSE11

R211=r2_score(vy,yp)

R211

errors = np.abs(vy - yp.ravel())
mean_true = np.mean(vy)
RAE11 = np.mean(errors) / mean_true

RAE11

ape = np.abs((vy - yp.ravel()) / vy)
MAPE11 = np.mean(ape) * 100

MAPE11





# 12 Sequential Method

model = Sequential()

model.add(SimpleRNN(64,activation='relu', input_shape=(7,1)))
model.add(Dense(units=1))

# 编译模型
model.compile(optimizer=Adam(), loss='mean_squared_error')

model.fit(tx,ty,epochs=150, batch_size=32)

yp=model.predict(vx)

MAE12=mean_absolute_error(vy,yp)

MAE12

MSE12=mean_squared_error(vy,yp)

MSE12

R212=r2_score(vy,yp)

R212

errors = np.abs(vy - yp.ravel())
mean_true = np.mean(vy)
RAE12 = np.mean(errors) / mean_true

RAE12

ape = np.abs((vy - yp.ravel()) / vy)
MAPE12 = np.mean(ape) * 100

MAPE12





# 13 Sequential GRU

model = Sequential()

model.add(GRU(64,activation='relu',input_shape= (7, 1)))
model.add(Dense(units=1))

# 编译模型
model.compile(optimizer=Adam(), loss='mean_squared_error')

model.fit(tx,ty,epochs=150, batch_size=32)

yp=model.predict(vx)

MAE13=mean_absolute_error(vy,yp)

MAE13

MSE13=mean_squared_error(vy,yp)

MSE13

R213=r2_score(vy,yp)

R213

errors = np.abs(vy - yp.ravel())
mean_true = np.mean(vy)
RAE13 = np.mean(errors) / mean_true

RAE13

ape = np.abs((vy - yp.ravel()) / vy)
MAPE13 = np.mean(ape) * 100

MAPE13





# 14 LSTM



import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow.keras.backend as K
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.callbacks import *
from tensorflow.keras.initializers import *
from tensorflow.keras.layers import *
from tensorflow.keras.models import *
from tensorflow.python.keras.layers import Layer


BATCH_SIZE = 32
EPOCHS = 150
FUTURE_PERIOD_PREDICT = 1


# Assuming X_train, y_train, X_test, and y_test are already defined
# by the previous data splitting cell
print('X_train', tx.shape,'y_train', ty.shape)
print('X_test', vx.shape,'y_test', vy.shape)


inp = Input(shape=(tx.shape[1], 1)) # Use the actual number of features and add a time step dimension
x = GRU(256, return_sequences=True)(inp)
x = Dropout(0.2)(x)
x = BatchNormalization()(x)

# Implement attention mechanism using standard Keras layers
# Compute attention scores
attention_scores = TimeDistributed(Dense(1, activation='tanh'))(x)
# Apply softmax to get attention weights
attention_weights = Softmax(axis=1)(attention_scores)
# Apply attention weights to the GRU output using element-wise multiplication
context_vector = x * attention_weights
# Sum the weighted inputs over time steps
context_vector = Lambda(lambda z: K.sum(z, axis=1))(context_vector)


x = Dense(32, activation="relu")(context_vector)
x = Dropout(0.2)(x)
x = Dense(1, activation="relu")(x)
model_lstm_attention = tf.keras.Model(inputs=inp, outputs=x)
model_lstm_attention.compile(loss='mean_squared_error', optimizer='adam')
model_lstm_attention.summary()

earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')

tf.keras.utils.plot_model(model_lstm_attention,
                          to_file="model_lstm_attention.png",
                          show_shapes=True)

model_lstm_attention.fit(tx, ty,
                    batch_size=BATCH_SIZE,
                    epochs=EPOCHS,
                    validation_data=(vx, vy),
                    callbacks=[earlyStopping]) # Added early stopping callback

# model_lstm_attention.save('lstm+gru.h5')

predicted_LSTM_Att = model_lstm_attention.predict(vx)

yp=predicted_LSTM_Att

from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score

MAE14=mean_absolute_error(vy,yp)

MAE14

MSE14=mean_squared_error(vy,yp)

MSE14

R214=r2_score(vy,yp)

R214

errors = np.abs(vy - yp.ravel())
mean_true = np.mean(vy)
RAE14 = np.mean(errors) / mean_true

RAE14

ape = np.abs((vy - yp.ravel()) / vy)
MAPE14 = np.mean(ape) * 100

MAPE14





# Now we compair our all resluts and ansure that which model is prefer for modeling.

final_result = pd.DataFrame({
    'Metric': ['MAE', 'MSE', 'R2', 'RAE', 'MAPE'],
    'Logistic Regression': [MAE1, MSE1, R21, RAE1, MAPE1],
    'Random Forest Regression': [MAE2, MSE2, R22, RAE2, MAPE2],
    'XG Boost': [MAE3, MSE3, R23, RAE3, MAPE3],
    'Bagging Regression': [MAE4, MSE4, R24, RAE4, MAPE4],
    'KNN Regression': [MAE5, MSE5, R25, RAE5, MAPE5],
    'Bayesian Ridge': [MAE6, MSE6, R26, RAE6, MAPE6],
    'Gaussian Process Regression': [MAE7, MSE7, R27, RAE7, MAPE7],
    'SVR': [MAE8, MSE8, R28, RAE8, MAPE8],
    'LGB': [MAE9, MSE9, R29, RAE9, MAPE9],
    'MLP Regression': [MAE10, MSE10, R210, RAE10, MAPE10],
    'Sequential': [MAE11, MSE11, R211, RAE11, MAPE11],
    'SimpleRNN': [MAE12, MSE12, R212, RAE12, MAPE12],
    'GRU': [MAE13, MSE13, R213, RAE13, MAPE13],
    'LSTM': [MAE14, MSE14, R214, RAE14, MAPE14]
})

display(final_result)